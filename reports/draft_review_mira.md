# Mira Draft Review of Hazel and Neel's Report

### Question:  What is your understanding of the experiment the team is replicating?  What question does it answer?  How clear is the team's explanation?

Explanation is clear.

### Methodology: Do you understand the methodology?  Does it make sense for the question?  Are there limitations you see that the team did not address?

I think your methodology makes sense. You were very clear about how you found the potential mistake in the original paper, which is good. One thing is you did note that you'll do the monte carlo simulation "if you have time". IMO you should definitely do this, as I could see the random aspect of your simulation causing a lot of spread in your data.

### Results: Do you understand what the results are (not yet considering their interpretation)?  If they are presented graphically, are the visualizations effective?  Do all figures have labels on the axes and captions?

It's not clear to me what the difference between some of the graphs in the results section are. These should have some title or caption to distinguish them. None of your figures have captions. Some of the figures have transparent backgrounds behind the text, which looks bad with GitHub dark mode.

I'll also note that the figure you show from the original paper isn't exactly the best figure IMO, so I would personally avoid replicating it exactly. I'd love to see a line graph of n on the x axis and percent on the y axis.

### Interpretation: Does the draft report interpret the results as an answer to the motivating question?  Does the argument hold water?

Your interpretation seems to focus more on narrowing in on the inconsistency in the original paper, rather than just the results alone. This isn't bad, but you should go deeper into your results.

### Replication: Are the results in the report consistent with the results from the original paper?  If so, how did the authors demonstrate that consistency?  Is it quantitative or qualitative?

I believe the results do replicate the original paper's findings, although you did find a potential mistake in the original paper. In that case, you have a well supported inconsistency, which IMO is also good.

### Extension: Does the report explain an extension to the original experiment clearly?  Can it answer an interesting question that the original experiment did not answer?

The extension seems natural and interesting to me

### Progress: Is the team roughly where they should be at this point, with a replication that is substantially complete and an extension that is clearly defined and either complete or nearly so?

I feel like you are slightly behind, as you don't say you have the extension substantially complete. This may be that you don't yet have results from this and didn't say how far along you were, rather than actually being behind.

### Presentation: Is the report written in clear, concise, correct language?  Is it consistent with the audience and goals of the report?  Does it violate any of the recommendations in my style guide (Links to an external site.)?

The presentation seems pretty good. The writing seems good. The figures could use some more work, such as color-coding, updating axis labels, titles, captions, etc.

### Mechanics: Is the report in the right directory with the right file name?  Is it formatted professionally in Markdown?  Does it include a meaningful title and the full names of the authors?  Is the bibliography in an acceptable style? 

Yes. One thing is the second paper doesn't have a link, which would be good.